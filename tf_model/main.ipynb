{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import subprocess, os\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['rnn_size'] = 400\n",
    "params['encoder_emb'] = 400\n",
    "params['decoder_emb'] = 400\n",
    "params['init_weights'] = 0.35\n",
    "params['dropout'] = 0.5\n",
    "params['normalize'] = 1\n",
    "params['beam_size'] = 10\n",
    "params['saved'] = None #Use saved model\n",
    "params['lr'] = 0.5\n",
    "params['dataset'] = 'Theano,youtube-dl,node,angular,react,opencv,CNTK,\\\n",
    "bitcoin,tensorflow,caffe,elasticsearch,guava'.split(',')\n",
    "params['language'] = 'python,python,javascript,javascript,javascript,cpp,cpp,cpp,cpp,cpp,java,java'.split(',')\n",
    "params['layers'] = 1\n",
    "params['decay'] = 0.8\n",
    "params['max_grad_norm'] = 5\n",
    "params['max_length'] = 20\n",
    "params['batch_size'] = 100\n",
    "params['max_code_length'] = 100\n",
    "params['max_nl_length'] = 100\n",
    "params['graph'] = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['train', 'valid', 'test']\n",
    "data = {}\n",
    "work_dir = os.environ['WORK_DIR']\n",
    "work_dir = os.path.join(work_dir,'preprocessing')\n",
    "dataset_name = '_'.join(params['dataset'])\n",
    "langauge_name = '_'.join(params['language'])\n",
    "\n",
    "for kind in data_types:\n",
    "    file_name = dataset_name + \".\" + langauge_name + \".\" + kind + \".npz\"\n",
    "    file_dir = os.path.join(work_dir,file_name)\n",
    "    data[kind] = np.load(file_dir, encoding='bytes')\n",
    "file_name = dataset_name + \".\" + langauge_name + \".\" + \"vocab.json\"\n",
    "file_dir = os.path.join(work_dir,file_name)\n",
    "with open(file_dir) as f:\n",
    "    data['vocab'] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['code_vocab_size'] = data['vocab']['max_code'] + 1\n",
    "params['nl_vocab_size'] = data['vocab']['max_nl'] + 1\n",
    "file_name = dataset_name + \".\" + langauge_name + \".ref.txt\"\n",
    "params['dev_ref_file'] = os.path.join(work_dir,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.npyio.NpzFile at 0x7f3e88514470>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train(ids):\n",
    "#     if type(ids)==list:\n",
    "    x_train = np.vstack([data['train']['X'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    y_train = np.vstack([data['train']['Y'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    xlen_train = np.hstack([data['train']['Xlen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "    ylen_train = np.hstack([data['train']['Ylen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1) \n",
    "#     else:\n",
    "#         x_train = data['train']['X'][()][params['dataset'][ids]].T\n",
    "#         y_train = data['train']['Y'][()][params['dataset'][ids]].T\n",
    "#         xlen_train = np.hstack((data['train']['Xlen'][()][params['dataset'][ids]])).reshape(-1)\n",
    "#         ylen_train = np.hstack((data['train']['Ylen'][()][params['dataset'][ids]])).reshape(-1) \n",
    "    return x_train, y_train, xlen_train, ylen_train\n",
    "def make_val_test(ids):\n",
    "#     if type(ids)==list:\n",
    "    x_valid = np.vstack([data['valid']['X'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    y_valid = np.vstack([data['valid']['Y'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    xlen_valid = np.hstack([data['valid']['Xlen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "    ylen_valid = np.hstack([data['valid']['Ylen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "    x_test = np.hstack([data['test']['X'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    y_test = np.hstack([data['test']['Y'][()][params['dataset'][idx]] for idx in ids]).T\n",
    "    xlen_test = np.hstack([data['test']['Xlen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "    ylen_test = np.hstack([data['test']['Ylen'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "    ids_test = np.hstack([data['test']['ids'][()][params['dataset'][idx]] for idx in ids]).reshape(-1)\n",
    "#     else:\n",
    "#         x_valid = data['valid']['X'][()][params['dataset'][ids]].T\n",
    "#         y_valid = data['valid']['Y'][()][params['dataset'][ids]].T\n",
    "#         xlen_valid = data['valid']['Xlen'][()][params['dataset'][ids]]\n",
    "#         ylen_valid = data['valid']['Ylen'][()][params['dataset'][ids]]\n",
    "#         x_test = data['test']['X'][()][params['dataset'][ids]].T\n",
    "#         y_test = data['test']['Y'][()][params['dataset'][ids]].T\n",
    "#         xlen_test = data['test']['Xlen'][()][params['dataset'][ids]]\n",
    "#         ylen_test = data['test']['Ylen'][()][params['dataset'][ids]]\n",
    "#         ids_test = data['test']['ids'][()][params['dataset'][ids]]\n",
    "    return x_valid, y_valid, xlen_valid, ylen_valid, x_test, y_test, xlen_test, ylen_test, ids_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = [9,10]\n",
    "valid_datasets = [7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, xlen_train, ylen_train = make_train(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid, y_valid, xlen_valid, ylen_valid, x_test, y_test, xlen_test, ylen_test, ids_test = make_val_test(valid_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_to_nl(prediction):\n",
    "    nl = \"\"\n",
    "    for word in prediction:\n",
    "        token = data['vocab']['num_to_nl'][str(word[0])]\n",
    "        if not token in ['UNK','CODE_START','CODE_END']: \n",
    "            nl += token\n",
    "            nl += \" \"\n",
    "    if nl == \"\":\n",
    "        return \"mysql\"\n",
    "    return nl[:-1]\n",
    "\n",
    "def run_bleu(predictions, ids):\n",
    "    os.chdir('/home/mohamed/tf-commitgen/')\n",
    "    with tempfile.NamedTemporaryFile(dir='/tmp', delete=False, mode='w') as tmp_file:\n",
    "        for i in range(ids.size):\n",
    "            tmp_file.write(str(ids[i]) + '\\t' + predictions[i] + '\\n')\n",
    "        tmp_name = tmp_file.name\n",
    "    bleu = subprocess.check_output([\"python\",\"./model/bleu.py\",\n",
    "                                    params['dev_ref_file']], stdin=open(tmp_name))\n",
    "    print(\"BLEU:\", str(bleu)[2:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, Xlen, Ylen):\n",
    "    #encoder embedding and cell\n",
    "    encoder_emb = tf.get_variable(\"encoder_emb\", [params['code_vocab_size'], params['encoder_emb']])\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(encoder_emb, X)\n",
    "    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(params['encoder_emb'])\n",
    "    #encoding\n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell, encoder_emb_inp, \n",
    "                                                                dtype=tf.float32,\n",
    "                                                               time_major=True, sequence_length=Xlen)\n",
    "    #decoder embedding and cell\n",
    "    decoder_emb = tf.get_variable(\"decoder_emb\", [params['nl_vocab_size'], params['rnn_size']])\n",
    "    decoder_emb_inp = tf.nn.embedding_lookup(decoder_emb, Y)\n",
    "    decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(params['decoder_emb'])\n",
    "    #dropout and attention\n",
    "    if params['graph'] == 'train':\n",
    "        decoder_cell = tf.contrib.rnn.DropoutWrapper(cell=decoder_cell, input_keep_prob=params['dropout'])\n",
    "    attention_states = tf.transpose(encoder_outputs, [1, 0, 2])\n",
    "    attention_mechanism = tf.contrib.seq2seq.LuongAttention(params['rnn_size'], attention_states,\n",
    "                                                            memory_sequence_length=Xlen)\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,attention_mechanism,\n",
    "                                                       attention_layer_size=params['rnn_size'])\n",
    "    #fix error due to LSTM state being a tuple \n",
    "    decoder_initial_state = decoder_cell.zero_state(tf.shape(Y)[1], tf.float32).clone(cell_state=encoder_state)\n",
    "    \n",
    "    #beam search\n",
    "    if params['graph'] == 'test':\n",
    "        decoder_initial_state = tf.contrib.seq2seq.tile_batch(decoder_initial_state, \n",
    "                                                              multiplier=params['beam_size'])\n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(decoder_cell, decoder_emb_inp, 3, 4,\n",
    "                                                   decoder_initial_state, beam,\n",
    "                                                   output_layer=projection_layer,\n",
    "                                                   length_penalty_weight=0.0)\n",
    "        outputs, _, ylen_out = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "        return outputs.predicted_ids, ylen_out\n",
    "        \n",
    "    #helper\n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(decoder_emb_inp, Ylen, time_major=True)\n",
    "    projection_layer = layers_core.Dense(params['nl_vocab_size'], use_bias=False)\n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, decoder_initial_state, output_layer=projection_layer)\n",
    "    \n",
    "    #decoding\n",
    "    outputs, _, ylen_out = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "    return outputs.rnn_output, ylen_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session, predict, predict_lengths, loss_val,\n",
    "              batch_size=100, print_every=100,\n",
    "              training=None, verbose = True, plot_losses=False):\n",
    "    predict = tf.argmax(predict,2)\n",
    "    \n",
    "    # have tensorflow compute accuracy\n",
    "    accuracy=tf.constant([0])\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(x_train.shape[1])\n",
    "    np.random.shuffle(train_indicies)\n",
    "    valid_indicies = np.arange(x_valid.shape[1])\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,predict,predict_lengths,accuracy]\n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    val_accs = []\n",
    "    e = 0\n",
    "    all_losses = []\n",
    "    best_acc = 0\n",
    "    while True:\n",
    "        e += 1\n",
    "        # keep track of losses and accuracy\n",
    "        accs = []\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(x_train.shape[1]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%x_train.shape[1]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            # get batch size\n",
    "            actual_batch_size = y_train[:,idx].shape[1]\n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: x_train[:,idx],\n",
    "                         Y: y_train[:,idx],\n",
    "                         Xlen: xlen_train[idx],\n",
    "                         Ylen: ylen_train[idx]}\n",
    "            params['graph'] = 'train'\n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            batch_loss, pred, pred_lengths, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            # aggregate performance stats\n",
    "            losses.append(batch_loss*actual_batch_size)\n",
    "            epoch_accs = []\n",
    "            for j,index in enumerate(idx):\n",
    "                sample_length = max(ylen_train[index], pred_lengths[j])\n",
    "                targets = y_train[:sample_length,index]\n",
    "                acc = np.mean(np.equal(targets, \n",
    "                                       pred[:sample_length,j]))\n",
    "                epoch_accs.append(acc)\n",
    "            acc = np.mean(epoch_accs)\n",
    "            #print every now and then\n",
    "            if verbose:\n",
    "                print('\\r[Training] %.2f%%' %(i/(int(math.ceil(x_train.shape[1]/batch_size))-1)*100), end='')\n",
    "            iter_cnt += 1\n",
    "            accs.append(acc) \n",
    "        print('\\r',end='')\n",
    "        total_acc = np.mean(accs)\n",
    "        total_loss = np.sum(losses)/x_train.shape[1] \n",
    "        \n",
    "        if plot_losses:\n",
    "            all_losses += losses\n",
    "            plt.plot(all_losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "            \n",
    "        for i in range(int(math.ceil(x_valid.shape[1]/batch_size))):\n",
    "            epoch_accs = []\n",
    "            start_idx = (i*batch_size)%x_valid.shape[1]\n",
    "            idx = valid_indicies[start_idx:start_idx+batch_size]\n",
    "            val_feed_dict = {X: x_valid[:,idx],\n",
    "                             Y: y_valid[:,idx],\n",
    "                             Xlen: xlen_valid[idx],\n",
    "                             Ylen: ylen_valid[idx]}\n",
    "            params['graph'] = 'valid'\n",
    "            pred, pred_lengths = session.run([predict,predict_lengths] , feed_dict=val_feed_dict)\n",
    "            iter_val_accs = []\n",
    "            for j,index in enumerate(idx):\n",
    "                sample_length = max(ylen_valid[index], pred_lengths[j])\n",
    "\n",
    "                acc = np.mean(np.equal(y_valid[:sample_length,index], \n",
    "                                       pred[:sample_length,j]))\n",
    "                iter_val_accs.append(acc)\n",
    "            acc = np.mean(iter_val_accs)\n",
    "            epoch_accs.append(acc)\n",
    "            if verbose:\n",
    "                print('[Validating] %.2f%%\\r' %(i/(int(math.ceil(x_valid.shape[1]/batch_size))-1)*100), flush=True, end='')\n",
    "        print('\\r',end='')\n",
    "        acc = np.mean(epoch_accs)\n",
    "        val_accs.append(acc)\n",
    "        if len(val_accs) > 10 and val_accs[-5] >= val_accs[-1]:\n",
    "            params['lr'] *= params['decay']\n",
    "            \n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g}, training accuracy of {1:.3g}, validation accuracy of {3: .3g} and learning rate = {4: .4g}\" \\\n",
    "              .format(total_loss,total_acc,e, val_accs[-1], params['lr']))\n",
    "        \n",
    "        \n",
    "#         if e>3:\n",
    "#             test_predictions = []\n",
    "#             for i in range(x_test.shape[0]):\n",
    "#                 feed_dict = {X: np.array(x_test[i]).reshape(-1,1),\n",
    "#                              Y: np.array(y_test[i]).reshape(-1,1),\n",
    "#                              Xlen: xlen_test[i].reshape(1),\n",
    "#                              Ylen: ylen_test[i].reshape(1)}\n",
    "#                 params['graph'] = 'test'\n",
    "#                 pred = session.run(predict,feed_dict=feed_dict)\n",
    "#                 test_predictions.append(num_to_nl(pred))\n",
    "#                 if verbose:\n",
    "#                     print('[Testing] %.2f%%\\r' %(i/(x_test.shape[0]-1)*100), flush=True, end='')\n",
    "#             print('\\r',end='')\n",
    "#             run_bleu(test_predictions, ids_test)\n",
    "#         if e>15:\n",
    "#             print(test_predictions[:20])\n",
    "        \n",
    "        if val_accs[-1] > best_acc:\n",
    "            save_path = work_dir + '/' + ''.join([str(x) for x in train_datasets]) + '.ckpt'\n",
    "            saver.save(session, save_path)\n",
    "            best_acc = val_accs[-1]\n",
    "            print('Model Saved (%.2f > %.2f)'%(val_accs[-1], best_acc))\n",
    "        if params['lr'] < 1e-3:\n",
    "            break\n",
    "    return total_loss,total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Overall loss = 1.53, training accuracy of 0.201, validation accuracy of  0.392 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 2, Overall loss = 0.748, training accuracy of 0.513, validation accuracy of  0.59 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 3, Overall loss = 0.448, training accuracy of 0.669, validation accuracy of  0.684 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 4, Overall loss = 0.313, training accuracy of 0.753, validation accuracy of  0.729 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 5, Overall loss = 0.23, training accuracy of 0.813, validation accuracy of  0.744 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 6, Overall loss = 0.161, training accuracy of 0.859, validation accuracy of  0.767 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 7, Overall loss = 0.11, training accuracy of 0.898, validation accuracy of  0.786 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 8, Overall loss = 0.0713, training accuracy of 0.929, validation accuracy of  0.808 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 9, Overall loss = 0.0456, training accuracy of 0.953, validation accuracy of  0.818 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 10, Overall loss = 0.0295, training accuracy of 0.969, validation accuracy of  0.82 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 11, Overall loss = 0.0204, training accuracy of 0.978, validation accuracy of  0.814 and learning rate =  0.005\n",
      "Epoch 12, Overall loss = 0.0153, training accuracy of 0.984, validation accuracy of  0.823 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 13, Overall loss = 0.0121, training accuracy of 0.988, validation accuracy of  0.823 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 14, Overall loss = 0.0104, training accuracy of 0.989, validation accuracy of  0.827 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 15, Overall loss = 0.00848, training accuracy of 0.991, validation accuracy of  0.83 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 16, Overall loss = 0.00693, training accuracy of 0.993, validation accuracy of  0.831 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 17, Overall loss = 0.00641, training accuracy of 0.993, validation accuracy of  0.834 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 18, Overall loss = 0.00585, training accuracy of 0.994, validation accuracy of  0.833 and learning rate =  0.005\n",
      "Epoch 19, Overall loss = 0.0051, training accuracy of 0.995, validation accuracy of  0.834 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 20, Overall loss = 0.00413, training accuracy of 0.996, validation accuracy of  0.84 and learning rate =  0.005\n",
      "Model Saved\n",
      "Epoch 21, Overall loss = 0.00367, training accuracy of 0.995, validation accuracy of  0.832 and learning rate =  0.004\n",
      "Epoch 22, Overall loss = 0.00348, training accuracy of 0.996, validation accuracy of  0.832 and learning rate =  0.0032\n",
      "Epoch 23, Overall loss = 0.00324, training accuracy of 0.996, validation accuracy of  0.837 and learning rate =  0.0032\n",
      "Epoch 24, Overall loss = 0.00322, training accuracy of 0.997, validation accuracy of  0.825 and learning rate =  0.00256\n",
      "Epoch 25, Overall loss = 0.00245, training accuracy of 0.997, validation accuracy of  0.848 and learning rate =  0.00256\n",
      "Model Saved\n",
      "Epoch 26, Overall loss = 0.00229, training accuracy of 0.997, validation accuracy of  0.839 and learning rate =  0.00256\n",
      "Epoch 27, Overall loss = 0.0026, training accuracy of 0.997, validation accuracy of  0.843 and learning rate =  0.00256\n",
      "Epoch 28, Overall loss = 0.00257, training accuracy of 0.997, validation accuracy of  0.841 and learning rate =  0.00256\n",
      "Epoch 29, Overall loss = 0.00235, training accuracy of 0.998, validation accuracy of  0.835 and learning rate =  0.002048\n",
      "Epoch 30, Overall loss = 0.00365, training accuracy of 0.996, validation accuracy of  0.84 and learning rate =  0.002048\n",
      "Epoch 31, Overall loss = 0.00405, training accuracy of 0.996, validation accuracy of  0.843 and learning rate =  0.001638\n",
      "[Training] 11.69%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-ff2f5b7629c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mylen_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-17371dd9abc8>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(session, predict, predict_lengths, loss_val, batch_size, print_every, training, verbose, plot_losses)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;31m# have tensorflow compute loss and correct predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# and (if given) perform a training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m             \u001b[0;31m# aggregate performance stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mactual_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "params['lr'] = 0.005\n",
    "params['saved'] = False\n",
    "params['graph'] = 'train'\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "Y = tf.placeholder(tf.int64, [None, None])\n",
    "Xlen = tf.placeholder(tf.int32, [None])\n",
    "Ylen = tf.placeholder(tf.int32, [None])\n",
    "y_out, ylen_out = model(X, Y, Xlen, Ylen)\n",
    "y_out = tf.transpose(y_out, [1,0,2])\n",
    "targets = Y[:tf.shape(y_out)[0]]\n",
    "targets = tf.one_hot(targets, params['nl_vocab_size'])\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = targets, logits = y_out)\n",
    "mean_loss = tf.reduce_mean(loss)\n",
    "\n",
    "trainable = tf.trainable_variables()\n",
    "gradients = tf.gradients(mean_loss, trainable)\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, params['max_grad_norm'])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(params['lr'])\n",
    "train_step = optimizer.apply_gradients(zip(clipped_gradients, trainable))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if params['saved']:\n",
    "        save_path = work_dir + '/' + ''.join([str(x) for x in train_datasets]) + '.ckpt'\n",
    "        saver.restore(sess, save_path)\n",
    "        print('Model Restored')\n",
    "    else:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    run_model(sess,y_out,ylen_out,mean_loss,params['batch_size'],100,train_step, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/mohamed/data/preprocessing/910.ckpt\n",
      "99.91%\n",
      "BLEU: 37.8696172544\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    save_path = work_dir + '/' + ''.join([str(x) for x in train_datasets]) + '.ckpt'\n",
    "    saver.restore(sess, save_path)\n",
    "    test_predictions = []\n",
    "    n = x_test.shape[0]\n",
    "    params['graph'] = 'test'\n",
    "    for i in range(n):\n",
    "        feed_dict = {X: np.array(x_test[i]).reshape(-1,1),\n",
    "                     Y: np.array(y_test[i]).reshape(-1,1),\n",
    "                     Xlen: xlen_test[i].reshape(1),\n",
    "                     Ylen: ylen_test[i].reshape(1)}\n",
    "        pred = sess.run(tf.argmax(y_out,2),feed_dict=feed_dict)\n",
    "        test_predictions.append(num_to_nl(pred))\n",
    "        print('\\r%.2f%%' %(i/n*100),end='',flush=True)\n",
    "    print()\n",
    "    run_bleu(test_predictions, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
