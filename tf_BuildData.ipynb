{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "from commitgen.data import build_data, split_list, build_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = os.environ['WORK_DIR']\n",
    "work_dir = os.path.join(work_dir, \"preprocessing\")\n",
    "if not os.path.isdir(work_dir):\n",
    "    os.mkdir(work_dir)\n",
    "\n",
    "nl_max_length = 100\n",
    "code_max_length = 100\n",
    "code_unk_threshold = 2\n",
    "nl_unk_threshold = 2\n",
    "test = True\n",
    "ratio = 0.8 \n",
    "\n",
    "\n",
    "datasets = ['Theano']\n",
    "languages = [\"python\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, test = False):\n",
    "    if test:\n",
    "        data = build_data(data, vocab)\n",
    "        X = []\n",
    "        Y = []\n",
    "        Ylen = []\n",
    "        Xlen = []\n",
    "        ids = []\n",
    "        for sample in data:\n",
    "            x = sample['code_num']\n",
    "            y = sample['nl_num']\n",
    "            ids.append(sample['id'])\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            Xlen.append(sample['code_sizes'])\n",
    "            Ylen.append(len(sample['nl_num']))\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "    else:\n",
    "        data = build_data(data, vocab,\n",
    "                          max_code_length=code_max_length,\n",
    "                          max_nl_length=nl_max_length)\n",
    "        X = np.array([])\n",
    "        ids = []\n",
    "        for sample in data:\n",
    "            x = np.ones(code_max_length)\n",
    "            np.put(x, range(sample['code_sizes']), sample['code_num'])\n",
    "            nl_num = sample['nl_num']\n",
    "            y = np.ones(nl_max_length)\n",
    "            np.put(y, range(len(nl_num)), nl_num)\n",
    "            ids.append(sample['id'])\n",
    "            if X.size == 0:\n",
    "                X = x\n",
    "                Y = y\n",
    "                Xlen = [sample['code_sizes']]\n",
    "                Ylen = [len(sample['nl_num'])]\n",
    "                continue\n",
    "            X = np.vstack((X, x))\n",
    "            Y = np.vstack((Y, y))\n",
    "            Xlen.append(sample['code_sizes'])\n",
    "            Ylen.append(len(sample['nl_num']))\n",
    "    return X, Y, np.array(Xlen), np.array(Ylen), np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size = 8177\n",
      "Total skipped = 40\n",
      "Successfully generated train data\n",
      "Total size = 923\n",
      "Total skipped = 3\n",
      "Successfully generated valid data\n",
      "Total size = 2304\n",
      "Total skipped = 0\n",
      "Total size = 2304\n",
      "Total skipped = 0\n",
      "Successfully generated test data\n"
     ]
    }
   ],
   "source": [
    "per_dataset_parsed_commits = []\n",
    "all_parsed_commits = []\n",
    "\n",
    "for dataset in datasets:\n",
    "  filepath = os.path.join(work_dir, dataset + \".pickle\")\n",
    "  if os.path.isfile(filepath):\n",
    "      with open(filepath, \"rb\") as f:\n",
    "          parsed_commits = pickle.load(f)\n",
    "      per_dataset_parsed_commits.append(parsed_commits)\n",
    "      all_parsed_commits += parsed_commits\n",
    "  else:\n",
    "    raise IOError(\"Pickle file does not exist\")\n",
    "\n",
    "vocab = build_vocab(all_parsed_commits, code_unk_threshold, nl_unk_threshold)\n",
    "\n",
    "dataset_name = \"_\".join(datasets)\n",
    "language_name =  \"_\".join(languages)\n",
    "\n",
    "# storing vocab\n",
    "vocab_file_name = \".\".join([dataset_name, language_name, 'vocab.json'])\n",
    "with open(path.join(work_dir, vocab_file_name), 'w') as f:\n",
    "    json.dump(vocab, f)\n",
    "\n",
    "per_dataset_train = []\n",
    "per_dataset_valid = []\n",
    "per_dataset_test = []\n",
    "all_test = []\n",
    "\n",
    "for parsed_commits in per_dataset_parsed_commits:\n",
    "    # splitting dataset\n",
    "    train, valid, test = split_list(parsed_commits, generate_test=test, ratio=ratio)\n",
    "    per_dataset_train.append(train)\n",
    "    per_dataset_valid.append(valid)\n",
    "    per_dataset_test.append(test)\n",
    "    all_test += test\n",
    "\n",
    "\n",
    "# generating data and saving files\n",
    "\n",
    "data_to_save = dict()\n",
    "data_to_save['X'] = dict()\n",
    "data_to_save['Y'] = dict()\n",
    "data_to_save['Xlen'] = dict()\n",
    "data_to_save['Ylen'] = dict()\n",
    "data_to_save['ids'] = dict()\n",
    "for i in range(len(datasets)):\n",
    "    dataset_X, dataset_Y, dataset_Xlen, dataset_Ylen, dataset_ids = process_data(per_dataset_train[i])\n",
    "    data_to_save['X'][datasets[i]] = dataset_X\n",
    "    data_to_save['Y'][datasets[i]] = dataset_Y\n",
    "    data_to_save['Xlen'][datasets[i]] = dataset_Xlen\n",
    "    data_to_save['Ylen'][datasets[i]] = dataset_Ylen\n",
    "\n",
    "train_name = \".\".join([dataset_name, language_name,\"train\"])\n",
    "np.savez(os.path.join(work_dir, train_name), \n",
    "         **data_to_save)\n",
    "print(\"Successfully generated train data\")\n",
    "\n",
    "data_to_save = dict()\n",
    "data_to_save['X'] = dict()\n",
    "data_to_save['Y'] = dict()\n",
    "data_to_save['Xlen'] = dict()\n",
    "data_to_save['Ylen'] = dict()\n",
    "data_to_save['ids'] = dict()\n",
    "for i in range(len(datasets)):\n",
    "    dataset_X, dataset_Y, dataset_Xlen, dataset_Ylen, dataset_ids = process_data(per_dataset_valid[i])\n",
    "    data_to_save['X'][datasets[i]] = dataset_X\n",
    "    data_to_save['Y'][datasets[i]] = dataset_Y\n",
    "    data_to_save['Xlen'][datasets[i]] = dataset_Xlen\n",
    "    data_to_save['Ylen'][datasets[i]] = dataset_Ylen\n",
    "\n",
    "valid_name = \".\".join([dataset_name, language_name, \"valid\"])\n",
    "np.savez(os.path.join(work_dir, valid_name), \n",
    "         **data_to_save)\n",
    "print(\"Successfully generated valid data\")\n",
    "\n",
    "\n",
    "\n",
    "# we don't set a maximum length ONLY for test data\n",
    "_, ref_data = build_data(all_test, vocab, ref=True)\n",
    "ref_name = \".\".join([dataset_name, language_name, \"ref.txt\"])\n",
    "with open(os.path.join(work_dir, ref_name), 'w') as f:\n",
    "    for sha, nl in ref_data:\n",
    "        try:\n",
    "            f.write(str(sha )+ \"\\t\" + nl.decode('utf-8').encode('ascii', 'ignore') + \"\\n\")\n",
    "        except:\n",
    "            f.write(str(sha )+ \"\\t\" + nl + \"\\n\")\n",
    "        \n",
    "data_to_save = dict()\n",
    "data_to_save['X'] = dict()\n",
    "data_to_save['Y'] = dict()\n",
    "data_to_save['Xlen'] = dict()\n",
    "data_to_save['Ylen'] = dict()\n",
    "data_to_save['ids'] = dict()\n",
    "for i in range(len(datasets)):\n",
    "    dataset_X, dataset_Y, dataset_Xlen, dataset_Ylen, dataset_ids = process_data(per_dataset_test[i], test)\n",
    "    data_to_save['X'][datasets[i]] = dataset_X\n",
    "    data_to_save['Y'][datasets[i]] = dataset_Y\n",
    "    data_to_save['Xlen'][datasets[i]] = dataset_Xlen\n",
    "    data_to_save['Ylen'][datasets[i]] = dataset_Ylen\n",
    "    data_to_save['ids'][datasets[i]] = dataset_ids\n",
    "\n",
    "test_name = \".\".join([dataset_name, language_name, \"test\"])\n",
    "np.savez(os.path.join(work_dir, test_name), \n",
    "         **data_to_save)\n",
    "print(\"Successfully generated test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8202, 100)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_to_save['']['Theano'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
